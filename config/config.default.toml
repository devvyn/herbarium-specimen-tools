# Engines register themselves via the ``herbarium.engines`` entry-point group.
# Third-party packages can expose engines with:
# [project.entry-points."herbarium.engines"]
# name = "pkg.module"
# ``preferred_engine`` must correspond to a registered engine name.
[ocr]
# Apple Vision first (95% accuracy), comprehensive cloud API fallbacks
preferred_engine = "vision"
# Prioritized engine order based on research findings and cost-effectiveness:
# 1. Apple Vision (95% accuracy, $0 cost, macOS only)
# 2. Budget APIs: Google Vision, Azure Vision ($1-1.50/1000)
# 3. Premium APIs: Claude, GPT-4o, Gemini ($2.50-15/1000)
# 4. Ultra-premium: GPT-4 Vision ($50/1000)
enabled_engines = ["vision", "google", "azure", "textract", "gemini", "claude", "gpt4o", "gpt4omini", "gpt"]
confidence_threshold = 0.80
# Comprehensive cloud provider coverage
require_api_fallback_on_windows = true
# Accepts ISO 639-1 or ISO 639-2 codes; engines normalize as needed.
langs = ["eng", "fra", "lat"]

[gpt]
# GPT-4 Vision (ultra-premium fallback)
fallback_threshold = 0.95
dry_run = false
model = "gpt-4-vision-preview"
prompt_dir = "prompts"
cost_per_1000 = 50.0

[gpt4o]
# GPT-4o Vision (faster, cheaper than GPT-4)
fallback_threshold = 0.85
model = "gpt-4o"
prompt_dir = "prompts"
cost_per_1000 = 2.5

[gpt4omini]
# GPT-4o-mini Vision (cost-effective, good accuracy)
fallback_threshold = 0.85
model = "gpt-4o-mini"
prompt_dir = "prompts"
cost_per_1000 = 0.15
# Note: Images use ~33x more tokens, effective cost similar to gpt-4o

[claude]
# Claude Vision API for high accuracy herbarium processing
model = "claude-3-5-sonnet-20241022"
fallback_threshold = 0.80
botanical_context = true
cost_per_1000 = 15.0

[google]
# Google Vision API - budget primary choice
credentials_path = ".google-credentials.json"
fallback_threshold = 0.70
cost_per_1000 = 1.5

[azure]
# Microsoft Azure Computer Vision - Windows-optimized
endpoint = "https://your-region.cognitiveservices.azure.com/"
subscription_key_env = "AZURE_COMPUTER_VISION_SUBSCRIPTION_KEY"
fallback_threshold = 0.70
cost_per_1000 = 1.0
# Features optimized for herbarium specimens
features = ["read", "ocr"]
detect_handwriting = true

[textract]
# AWS Textract - document analysis focused
region = "us-east-1"
fallback_threshold = 0.75
cost_per_1000 = 1.5
# Use document analysis for complex layouts
analyze_document = true
feature_types = ["TABLES", "FORMS", "SIGNATURES"]

[gemini]
# Google Gemini Vision - latest multimodal AI
model = "gemini-pro-vision"
fallback_threshold = 0.80
cost_per_1000 = 2.5
# Enhanced for scientific content
safety_settings = "block_few"

[gemini.generation_config]
temperature = 0.1
top_p = 0.8
top_k = 40
max_output_tokens = 1024

[pipeline]
steps = ["image_to_text", "text_to_dwc"]

[storage]
# Storage backend configuration (OPTIONAL)
# If omitted, uses local filesystem with --input directory
# backend = "local"  # Options: "local", "s3", "minio", "http"

# Local filesystem configuration (default if no backend specified)
# base_path = "/path/to/images"  # Base directory for images

# Caching configuration (for remote backends)
# cache_enabled = true
# cache_dir = "/tmp/herbarium-image-cache"
# cache_max_size_mb = 1000  # Optional maximum cache size

# S3 backend configuration (uncomment to use AWS S3)
# [storage.s3]
# bucket = "my-herbarium-images"
# prefix = "specimens/"  # Optional S3 key prefix
# region = "us-east-1"
# access_key_id = "..."  # Optional (uses AWS credentials if omitted)
# secret_access_key = "..."  # Optional (uses AWS credentials if omitted)

# MinIO backend configuration (S3-compatible object storage)
# [storage.minio]
# endpoint = "http://localhost:9000"
# bucket = "herbarium-images"
# prefix = "specimens/"  # Optional prefix
# access_key = "minioadmin"
# secret_key = "minioadmin"
# region = "us-east-1"  # Usually not needed for MinIO

[preprocess]
pipeline = ["grayscale", "deskew", "binarize", "resize"]
binarize_method = "adaptive"  # "otsu" or "adaptive"
max_dim_px = 4000
contrast_factor = 1.5  # used when "contrast" is in the pipeline

[dwc]
schema = "dwc-abcd"
schema_uri = "http://rs.tdwg.org/dwc/terms/"
schema_files = ["dwc.xsd", "abcd.xsd"]
# Schema source configuration
use_official_schemas = false  # Set to true to fetch from official TDWG sources
preferred_official_schemas = ["dwc_simple", "abcd_206"]  # Schema names to use when fetching official schemas
schema_cache_enabled = true  # Enable caching of downloaded schemas
schema_update_interval_days = 30  # How often to check for schema updates
schema_compatibility_check = true  # Validate terms against target schemas
assume_country_if_missing = "Canada"
strict_minimal_fields = ["catalogNumber","scientificName","eventDate","recordedBy","locality"]
flag_if_missing_minimal = true
parse_scientific_names = true
normalize_spelling = true
do_not_update_nomenclature = true

[dwc.custom]
# Add custom field mappings. Example:
# barcode = "catalogNumber"  # raw_field = "dwc_term"

[qc]
dupes = ["catalog","sha256","phash"]
phash_threshold = 10
low_confidence_flag = true
top_fifth_scan_pct = 20

[qc.gbif]
enabled = false
# Primary GBIF API endpoints
species_match_endpoint = "https://api.gbif.org/v1/species/match"
reverse_geocode_endpoint = "https://api.gbif.org/v1/geocode/reverse"
occurrence_search_endpoint = "https://api.gbif.org/v1/occurrence/search"
suggest_endpoint = "https://api.gbif.org/v1/species/suggest"

# Network and retry configuration
timeout = 10.0
retry_attempts = 3
backoff_factor = 1.0
cache_size = 1000

# Verification behavior settings
enable_fuzzy_matching = true
min_confidence_score = 0.80
enable_occurrence_validation = false

# Quality control thresholds
max_coordinate_distance_km = 10.0
min_occurrence_count_threshold = 1

[report]
html = true

[processing]
retry_limit = 3

[export]
# Darwin Core Archive export configuration
enable_versioned_exports = true
default_export_version = "1.0.0"
bundle_format = "rich"  # "rich" (with metadata) or "simple" (version only)
include_checksums = true
include_git_info = true
include_system_info = true
auto_increment_version = false  # Automatically increment patch version
export_retention_days = 365  # How long to keep old exports (0 = keep forever)

# Additional files to include in archives (relative to output directory)
additional_files = []  # Example: ["README.txt", "processing_log.txt"]

# Export filename patterns
rich_filename_pattern = "dwca_{version}_{timestamp}_{commit}_{filter_hash}.zip"
simple_filename_pattern = "dwca_v{version}.zip"
